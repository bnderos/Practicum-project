## practic_2     
### Сверточные сети. Алгоритм Adam.    
Градиентный спуск (SGD) — это не самый оптимальный алгоритм обучения нейронной сети. Если величина шага слишком маленькая, сеть будет обучаться долго, а если большая — может пропустить минимум. Чтобы подбор шага был автоматическим, применяют алгоритм Adam (от англ. adaptive moment estimation, «адаптивность на основе оценки моментов»). Он подбирает различные параметры для разных нейронов, что также ускоряет обучение модели.

## practic_3    
### Свёрточные сети для классификации фруктов   

## practic_4    
### ResNet в Keras    
Архитектура ResNet возникла, чтобы решить проблему затухающего градиента в очень глубоких сетях. Благодаря ей обучать 100 слоёв не составляет труда. Есть даже примеры обучаемых сетей глубиной более 400 слоёв. Главная особенность ResNet заключается в использовании Shortcut Connections — дополнительные связи внутри сети, которые позволяют избежать проблемы затухающего градиента.    
